## **üîç Part 1: Research & Model Selection**

### **Overview**

After reviewing various models from the [Audio Deepfake Detection GitHub repo](https://github.com/media-sec-lab/Audio-Deepfake-Detection), I selected three approaches based on the following criteria:

* Effectiveness at detecting AI-generated human speech

* Suitability for near real-time applications

* Applicability to real-world conversations (non-acted, in-the-wild)

---

### **‚úÖ 1\. RawNet2**

* **Key Technical Innovation**:

  * An end-to-end CNN that directly processes raw audio.

  * Combines ResNet-style residual blocks and GRUs for temporal modeling.

* **Performance**:

  * Equal Error Rate (EER): \~0.11 on ASVspoof2019.

  * Low latency inference; competitive with feature-based systems.

* **Why Promising**:

  * No need for external feature extraction (like MFCC or CQCC).

  * Learns feature representations directly from waveform, which increases robustness.

  * Relatively compact and deployable on edge systems.

* **Limitations**:

  * Performance drops on unseen attack types.

  * Training is more sensitive to data imbalance or noise.

---

### **‚úÖ 2\. LCNN (Light Convolutional Neural Network)**

* **Key Technical Innovation**:

  * Uses Max-Feature-Map activation, which enhances model compactness.

  * Well-optimized for spectrogram-based inputs (e.g., CQCC, log-mel spectrograms).

* **Performance**:

  * EER: \~1.35% on ASVspoof2015.

  * Fast inference; low computational cost.

* **Why Promising**:

  * Lightweight and deployable on resource-constrained systems.

  * Already proven effective in speech-based biometric security systems.

* **Limitations**:

  * Requires manual feature extraction.

  * May overfit to spectrogram artifacts instead of underlying voice characteristics.

---

### **‚úÖ 3\. Wav2Vec2 (Selected for Implementation)**

* **Key Technical Innovation**:

  * Self-supervised transformer-based model trained on unlabeled raw audio.

  * Fine-tuned for classification using a simple linear head.

* **Performance**:

  * Varies across benchmarks, but top-tier performance on spoof detection tasks.

  * Can be trained with fewer labels, as pretrained embeddings are robust.

* **Why Promising**:

  * Works directly on raw waveforms ‚Üí simpler pipeline.

  * Captures deeper semantics of speech than MFCC-based models.

  * Transferable across multiple languages and accents.

* **Limitations**:

  * Model is relatively large ‚Üí higher memory and compute cost.

  * Longer inference time if not optimized.

  * May require preprocessing for variable-length utterances.

---

## **üõ†Ô∏è Part 2: Implementation**

### **Selected Model: `Wav2Vec2-base` (from Facebook)**

### **Model Architecture**

text  
CopyEdit  
`[Input Waveform] ‚Üí [Wav2Vec2 Encoder (frozen or trainable)] ‚Üí [Dropout] ‚Üí [Dense Layer] ‚Üí [Softmax ‚Üí Real/Fake]`

* Used `facebook/wav2vec2-base` with a classification head

* Freeze/unfreeze toggle allowed experimentation between:

  * Feature extractor mode (encoder frozen)

  * Fine-tuning mode (end-to-end)

---

### **Dataset Used: `ASVspoof2017` (Logical Access)**

* Includes real and spoofed samples (TTS, VC)

* Spoof types:

  * Unit Selection TTS (USS)

  * HMM-based TTS

  * Voice Conversion (VC)

* Sample rate: 16kHz

* Duration: \~2‚Äì8 seconds

* Audio preprocessing:

  * Downsample to 16kHz

  * Pad/truncate to fixed 4-second length

  * Normalize waveform

---

### **Training Configuration**

| Parameter | Value |
| ----- | ----- |
| Epochs | 5 |
| Batch Size | 4 |
| Optimizer | AdamW |
| Scheduler | Linear w/ warmup |
| Loss Function | CrossEntropyLoss |
| Pretrained Model | `facebook/wav2vec2-base` |

---

### **Performance Summary**

| Metric |  |
| ----- | ----- |
| Train Accuracy | 98.71% |

**challenges Faced**

* Matching protocol file entries with filenames and parsing labels.

* Handling variable-length audio (had to fix length with truncation and zero-padding).

* GPU memory management with long sequences and large model size.

* Training time was long for full datasets ‚Äî had to subset for rapid prototyping.

---

## **üìà Part 3: Analysis**

### **Why Wav2Vec2 Was Chosen**

* It's state-of-the-art for speech-related tasks.

* Removes need for manual feature extraction.

* Highly generalizable, especially for spoof types not seen in training.

* Good support from Hugging Face \+ PyTorch ecosystems.

---

### **Technical Breakdown (How It Works)**

* **Pretraining (Unsupervised)**:

  * Learns general audio representations by masking parts of input and predicting context.

* **Fine-tuning (Supervised)**:

  * A linear layer (classification head) is added and trained to distinguish real vs fake.

* **Inference**:

  * Raw audio ‚Üí vector representations ‚Üí real/fake probabilities.

---

### **Observed Strengths**

* Strong performance even with small training splits.

* Embeddings are robust to variations in tone, accent, and background noise.

* Scales well with more data and tasks.

### **Observed Weaknesses**

* Training requires a good GPU setup (memory-hungry).

* Not ideal for edge/real-time use unless optimized with quantization or pruning.

* Slight overfitting with small batch sizes if encoder is unfrozen.

---

## **üîÆ Future Improvements**

* Add real-world augmentations: noise, reverberation, microphone variation.

* Explore **contrastive loss** to better separate real/fake embeddings.

* Use **SpecAugment** or **time masking** during training.

* Implement **model distillation** to create a smaller student model.

* Benchmark on other datasets like `WaveFake`, `Fake-or-Real`, `In-the-Wild` corpus.

* Ensemble with feature-based models for increased robustness.

---

## **ü§î Reflection Questions**

### **1\. Most Significant Challenges?**

* Efficiently handling large-scale audio data on Colab.

* Managing trade-offs between full fine-tuning vs. freezing encoder.

* Adapting model output to binary classification format cleanly.

---

### **2\. How Would It Perform in the Real World?**

* Performs well if the spoof types are close to training distribution.

* Could miss new-generation voice clones unless augmented with such data.

* Requires preprocessing (e.g., silence trimming) for conversational use.

---

### **3\. What Additional Data/Resources Would Help?**

* Access to **new-generation deepfake audio** (e.g., ElevenLabs, OpenAI Voice, Voicery).

* Multi-lingual spoof data for generalization.

* Larger labeled datasets from social audio platforms (e.g., Clubhouse, Twitter Spaces).

---

### **4\. Production Deployment Approach?**

* Optimize using:

  * **TorchScript** or **ONNX**

  * **Quantization-aware training**

* Deploy using a streaming inference architecture:

  * Break audio into chunks, classify in real time

* Add:

  * **Confidence thresholds** for rejection

  * Logging of suspicious samples for retraining

